{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13c8abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a ChatBot class with all necessary modules to make a complete conversation\n",
    "class ChatBot():\n",
    "    # initialize\n",
    "    def __init__(self):\n",
    "        # once chat starts, the history will be stored for chat continuity\n",
    "        self.chat_history_ids = None\n",
    "        # make input ids global to use them anywhere within the object\n",
    "        self.bot_input_ids = None\n",
    "        # a flag to check whether to end the conversation\n",
    "        self.end_chat = False\n",
    "        \n",
    "    def user_input(self):\n",
    "        self.end_chat = False\n",
    "        # receive input from user\n",
    "        text = input_widget.value\n",
    "        input_widget.value = \"\"\n",
    "        output_widget.value += f\"<p><strong>You:</strong> {text}</p>\"\n",
    "        text = text.lower()\n",
    "        # end conversation if user wishes so\n",
    "        if text.strip() in ['bye', 'quit', 'exit']:\n",
    "            # turn flag on \n",
    "            self.end_chat = True\n",
    "            # a closing comment\n",
    "            output_widget.value += f\"<p><strong>ChatBot:</strong> See you soon! Bye!</p>\"\n",
    "        else:\n",
    "            # continue chat, preprocess input text\n",
    "            # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "            self.new_user_input_ids = tokenizer.encode(text + tokenizer.eos_token, \\\n",
    "                                                       return_tensors='pt')\n",
    "\n",
    "    def bot_response(self):\n",
    "        if self.end_chat == False:\n",
    "            # append the new user input tokens to the chat history\n",
    "            # if chat has already begun\n",
    "            if self.chat_history_ids is not None:\n",
    "                self.bot_input_ids = torch.cat([self.chat_history_ids, self.new_user_input_ids], dim=-1) \n",
    "            else:\n",
    "                # if first entry, initialize bot_input_ids\n",
    "                self.bot_input_ids = self.new_user_input_ids\n",
    "\n",
    "            # define the new chat_history_ids based on the preceding chats\n",
    "            # generated a response while limiting the total chat history to 1000 tokens, \n",
    "            self.chat_history_ids = model.generate(self.bot_input_ids, max_length=1000, \\\n",
    "                                                   pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "            # last ouput tokens from bot\n",
    "            response = tokenizer.decode(self.chat_history_ids[:, self.bot_input_ids.shape[-1]:][0], \\\n",
    "                                   skip_special_tokens=True)\n",
    "            # in case, bot fails to answer\n",
    "            if response == \"\":\n",
    "                response = self.random_response()\n",
    "            # print bot response\n",
    "            output_widget.value += f\"<p><strong>ChatBot:</strong> {response}</p>\"\n",
    "        \n",
    "    # in case there is no response from model\n",
    "    def random_response(self):\n",
    "        i = -1\n",
    "        response = tokenizer.decode(self.chat_history_ids[:, self.bot_input_ids.shape[i]:][0], \\\n",
    "                               skip_special_tokens=True)\n",
    "        # iterate over history backwards to find the last token\n",
    "        while response == '':\n",
    "            i = i-1\n",
    "            response = tokenizer.decode(self.chat_history_ids[:, self.bot_input_ids.shape[i]:][0], \\\n",
    "                               skip_special_tokens=True)\n",
    "        # if it is a question, answer suitably\n",
    "        if response.strip() == '?':\n",
    "            reply = np.random.choice([\"I don't know\", \n",
    "                                     \"I am not sure\"])\n",
    "        # not a question? answer suitably\n",
    "        else:\n",
    "            reply = np.random.choice([\"Great\", \n",
    "                                      \"Fine. What's up?\", \n",
    "                                      \"Okay\"\n",
    "                                     ])\n",
    "        return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8a8d08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
